{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Graph utilities.\"\"\"\n",
    "\n",
    "# from time import time\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "__author__ = \"Zhang Zhengyan\"\n",
    "__email__ = \"zhangzhengyan14@mails.tsinghua.edu.cn\"\n",
    "\n",
    "\n",
    "class Graph(object):\n",
    "    def __init__(self):\n",
    "        self.G = None\n",
    "        self.look_up_dict = {}\n",
    "        self.look_back_list = []\n",
    "        self.node_size = 0\n",
    "\n",
    "    def encode_node(self):\n",
    "        look_up = self.look_up_dict\n",
    "        look_back = self.look_back_list\n",
    "        \n",
    "        for node in self.G.nodes():\n",
    "            look_up[node] = self.node_size\n",
    "            look_back.append(node)\n",
    "            self.node_size += 1\n",
    "            self.G.nodes[node]['status'] = ''\n",
    "\n",
    "    def read_g(self, g):\n",
    "        self.G = g\n",
    "        self.encode_node()\n",
    "\n",
    "    def read_adjlist(self, filename):\n",
    "        \"\"\" Read graph from adjacency file in which the edge must be unweighted\n",
    "            the format of each line: v1 n1 n2 n3 ... nk\n",
    "            :param filename: the filename of input file\n",
    "        \"\"\"\n",
    "        self.G = nx.read_adjlist(filename, create_using=nx.DiGraph())\n",
    "       \n",
    "        for i, j in self.G.edges():\n",
    "            self.G[i][j]['weight'] = 1.0\n",
    "        self.encode_node()\n",
    "\n",
    "    def read_edgelist(self, filename, weighted=False, directed=False):\n",
    "        self.G = nx.DiGraph()\n",
    "\n",
    "        if directed:\n",
    "            def read_unweighted(l):\n",
    "                src, dst = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G[src][dst]['weight'] = 1.0\n",
    "\n",
    "            def read_weighted(l):\n",
    "                src, dst, w = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G[src][dst]['weight'] = float(w)\n",
    "        else:\n",
    "            def read_unweighted(l):\n",
    "                src, dst = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G.add_edge(dst, src)\n",
    "                self.G[src][dst]['weight'] = 1.0\n",
    "                self.G[dst][src]['weight'] = 1.0\n",
    "\n",
    "            def read_weighted(l):\n",
    "                src, dst, w = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G.add_edge(dst, src)\n",
    "                self.G[src][dst]['weight'] = float(w)\n",
    "                self.G[dst][src]['weight'] = float(w)\n",
    "        fin = open(filename, 'r')\n",
    "        func = read_unweighted\n",
    "        if weighted:\n",
    "            func = read_weighted\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            func(l)\n",
    "        fin.close()\n",
    "        self.encode_node()\n",
    "\n",
    "    def read_node_label(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            vec = l.split()\n",
    "            self.G.nodes[vec[0]]['label'] = vec[1:]\n",
    "        fin.close()\n",
    "\n",
    "    def read_node_features(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        for l in fin.readlines():\n",
    "            vec = l.split()\n",
    "            self.G.nodes[vec[0]]['feature'] = np.array(\n",
    "                [float(x) for x in vec[1:]])\n",
    "        fin.close()\n",
    "\n",
    "    def read_node_status(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            vec = l.split()\n",
    "            self.G.nodes[vec[0]]['status'] = vec[1]  # train test valid\n",
    "        fin.close()\n",
    "\n",
    "    def read_edge_label(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            vec = l.split()\n",
    "            self.G[vec[0]][vec[1]]['label'] = vec[2:]\n",
    "        fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "\n",
    "\n",
    "class TADW(object):\n",
    "\n",
    "    def __init__(self, graph, dim, lamb=0.2):\n",
    "        self.g = graph\n",
    "        self.lamb = lamb\n",
    "        self.dim = int(dim/2)\n",
    "        self.train()\n",
    "\n",
    "    def getAdj(self):\n",
    "        graph = self.g.G\n",
    "        node_size = self.g.node_size\n",
    "        look_up = self.g.look_up_dict\n",
    "        adj = np.zeros((node_size, node_size))\n",
    "        for edge in self.g.G.edges():\n",
    "            adj[look_up[edge[0]]][look_up[edge[1]]] = 1.0\n",
    "            adj[look_up[edge[1]]][look_up[edge[0]]] = 1.0\n",
    "        # ScaleSimMat\n",
    "        return adj/np.sum(adj, axis=1)\n",
    "\n",
    "    def save_embeddings(self, filename):\n",
    "        fout = open(filename, 'w')\n",
    "        node_num = len(self.vectors.keys())\n",
    "        fout.write(\"{} {}\\n\".format(node_num, self.dim*2))\n",
    "        for node, vec in self.vectors.items():\n",
    "            fout.write(\"{} {}\\n\".format(node, ' '.join([x.__str__() for x in vec])))\n",
    "        fout.close()\n",
    "\n",
    "    def getOriginalT(self):\n",
    "        g = self.g.G\n",
    "        look_back = self.g.look_back_list\n",
    "        self.features = np.vstack([g.nodes[look_back[i]]['feature']\n",
    "                                   for i in range(g.number_of_nodes())])\n",
    "        #self.features = self.features / (np.sum(self.features , axis=1)).reshape(len(self.features),1)\n",
    "        return self.features.T\n",
    "    \n",
    "    def getT(self):\n",
    "        g = self.g.G\n",
    "        look_back = self.g.look_back_list\n",
    "        \n",
    "        \n",
    "                \n",
    "        self.features = np.vstack([g.nodes[look_back[i]]['feature']\n",
    "                                   for i in range(g.number_of_nodes())])\n",
    "        self.preprocessFeature()\n",
    "        return self.features.T\n",
    "\n",
    "    def preprocessFeature(self):\n",
    "        if self.features.shape[1] > 50:\n",
    "            U, S, VT = la.svd(self.features)\n",
    "            Ud = U[:, 0:50]\n",
    "            Sd = S[0:50]\n",
    "            self.features = np.array(Ud)*Sd.reshape(50)\n",
    "            \n",
    "    def getM(self):\n",
    "        self.adj = self.getAdj()\n",
    "        return (self.adj + np.dot(self.adj, self.adj ))/2\n",
    "    \n",
    "    def train(self):\n",
    "        self.adj = self.getAdj()\n",
    "        # M=(A+A^2)/2 where A is the row-normalized adjacency matrix\n",
    "        self.M = (self.adj + np.dot(self.adj, self.adj) )/ 2\n",
    "        # T is feature_size*node_num, text features\n",
    "        self.T = self.getT()\n",
    "        self.node_size = self.adj.shape[0]\n",
    "        self.feature_size = self.features.shape[1]\n",
    "        self.W = np.random.randn(self.dim, self.node_size)\n",
    "        self.H = np.random.randn(self.dim, self.feature_size)\n",
    "        \n",
    "        print(np.sum(self.W))\n",
    "        print(np.sum(self.H))\n",
    "        # Update\n",
    "        for i in range(50):\n",
    "            print('Iteration ', i)\n",
    "            # Update W\n",
    "            B = np.dot(self.H, self.T)\n",
    "            drv = 2 * np.dot(np.dot(B, B.T), self.W) - \\\n",
    "                2*np.dot(B, self.M.T) + self.lamb*self.W\n",
    "            Hess = 2*np.dot(B, B.T) + self.lamb*np.eye(self.dim)\n",
    "            drv = np.reshape(drv, [self.dim*self.node_size, 1])\n",
    "            rt = -drv\n",
    "            dt = rt\n",
    "            vecW = np.reshape(self.W, [self.dim*self.node_size, 1])\n",
    "            while np.linalg.norm(rt, 2) > 1e-4:\n",
    "                dtS = np.reshape(dt, (self.dim, self.node_size))\n",
    "                Hdt = np.reshape(np.dot(Hess, dtS), [\n",
    "                                 self.dim*self.node_size, 1])\n",
    "\n",
    "                at = np.dot(rt.T, rt)/np.dot(dt.T, Hdt)\n",
    "                vecW = vecW + at*dt\n",
    "                rtmp = rt\n",
    "                rt = rt - at*Hdt\n",
    "                bt = np.dot(rt.T, rt)/np.dot(rtmp.T, rtmp)\n",
    "                dt = rt + bt * dt\n",
    "            self.W = np.reshape(vecW, (self.dim, self.node_size))\n",
    "\n",
    "            # Update H\n",
    "            drv = np.dot((np.dot(np.dot(np.dot(self.W, self.W.T), self.H), self.T)\n",
    "                          - np.dot(self.W, self.M.T)), self.T.T) + self.lamb*self.H\n",
    "            drv = np.reshape(drv, (self.dim*self.feature_size, 1))\n",
    "            rt = -drv\n",
    "            dt = rt\n",
    "            vecH = np.reshape(self.H, (self.dim*self.feature_size, 1))\n",
    "            while np.linalg.norm(rt, 2) > 1e-4:\n",
    "                dtS = np.reshape(dt, (self.dim, self.feature_size))\n",
    "                Hdt = np.reshape(np.dot(np.dot(np.dot(self.W, self.W.T), dtS), np.dot(self.T, self.T.T))\n",
    "                                 + self.lamb*dtS, (self.dim*self.feature_size, 1))\n",
    "                at = np.dot(rt.T, rt)/np.dot(dt.T, Hdt)\n",
    "                vecH = vecH + at*dt\n",
    "                rtmp = rt\n",
    "                rt = rt - at*Hdt\n",
    "                bt = np.dot(rt.T, rt)/np.dot(rtmp.T, rtmp)\n",
    "                dt = rt + bt * dt\n",
    "            self.H = np.reshape(vecH, (self.dim, self.feature_size))\n",
    "        self.Vecs = np.hstack(\n",
    "            (normalize(self.W.T), normalize(np.dot(self.T.T, self.H.T))))\n",
    "        # get embeddings\n",
    "        self.vectors = {}\n",
    "        look_back = self.g.look_back_list\n",
    "        for i, embedding in enumerate(self.Vecs):\n",
    "            self.vectors[look_back[i]] = embedding  \n",
    "        print(np.sum(self.W))\n",
    "        print(np.sum(self.H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import ast\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "t1 = time.time()\n",
    "g = Graph()\n",
    "print(\"Reading...\")\n",
    "\n",
    "filename = 'Foursquare/reorder_train.txt'\n",
    "g.read_adjlist(filename)\n",
    "\n",
    "feature_file = 'Foursquare/GMM_tadw_embed.txt'\n",
    "g.read_node_features(feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-168.92841732074902\n",
      "-107.94284405635781\n",
      "Iteration  0\n",
      "Iteration  1\n",
      "Iteration  2\n",
      "Iteration  3\n",
      "Iteration  4\n",
      "Iteration  5\n",
      "Iteration  6\n",
      "Iteration  7\n",
      "Iteration  8\n",
      "Iteration  9\n",
      "Iteration  10\n",
      "Iteration  11\n",
      "Iteration  12\n",
      "Iteration  13\n",
      "Iteration  14\n",
      "Iteration  15\n",
      "Iteration  16\n",
      "Iteration  17\n",
      "Iteration  18\n",
      "Iteration  19\n",
      "Iteration  20\n",
      "Iteration  21\n",
      "Iteration  22\n",
      "Iteration  23\n",
      "Iteration  24\n",
      "Iteration  25\n",
      "Iteration  26\n",
      "Iteration  27\n",
      "Iteration  28\n",
      "Iteration  29\n",
      "Iteration  30\n",
      "Iteration  31\n",
      "Iteration  32\n",
      "Iteration  33\n",
      "Iteration  34\n",
      "Iteration  35\n",
      "Iteration  36\n",
      "Iteration  37\n",
      "Iteration  38\n",
      "Iteration  39\n",
      "Iteration  40\n",
      "Iteration  41\n",
      "Iteration  42\n",
      "Iteration  43\n",
      "Iteration  44\n",
      "Iteration  45\n",
      "Iteration  46\n",
      "Iteration  47\n",
      "Iteration  48\n",
      "Iteration  49\n",
      "-28.00677102750059\n",
      "0.13669098308975058\n"
     ]
    }
   ],
   "source": [
    "representation_size = 200\n",
    "lamb = 0.2\n",
    "model = TADW( graph=g, dim=representation_size, lamb=lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'Foursquare/tadw_embed.txt'\n",
    "\n",
    "#model.Vecs = np.hstack((normalize(model.W.T), normalize(np.dot(model.T.T, model.H.T))))\n",
    "#model.Vecs = np.hstack( (normalize(model.W.T),normalize(model.T.T)) )\n",
    "#model.Vecs = normalize(np.dot(model.T.T, model.H.T))\n",
    "#model.Vecs = normalize(model.W.T)\n",
    "#model.Vecs = model.T.T\n",
    "#model.Vecs = model.W.T\n",
    "model.Vecs = np.hstack((model.W.T, np.dot(model.T.T, model.H.T)))\n",
    "#model.Vecs = np.dot(model.T.T, model.H.T)\n",
    "\n",
    "model.vectors = {}\n",
    "look_back = model.g.look_back_list\n",
    "for i, embedding in enumerate(model.Vecs):\n",
    "    model.vectors[look_back[i]] = embedding\n",
    "\n",
    "fout = open(filename, 'w')\n",
    "for vkey in model.vectors.keys():\n",
    "    \n",
    "    fout.write(\"{}\\n\".format( ' '.join([str(i) for i in model.vectors[vkey]]) ))\n",
    "        \n",
    "fout.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(model.vectors['121']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc value: 0.6944960776283041\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "node2vec = {}\n",
    "\n",
    "f = open('Foursquare/tadw_embed.txt', 'rb')\n",
    "for i, j in enumerate(f):\n",
    "    if j.decode() != '\\n':\n",
    "        node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
    "        \n",
    "f1 = open(os.path.join('Foursquare/reorder_test.txt'), 'rb')\n",
    "edges = [list(map(int, i.strip().decode().split('\\t'))) for i in f1]\n",
    "nodes = list(set([i for j in edges for i in j]))\n",
    "a = 0\n",
    "b = 0\n",
    "for i, j in edges:\n",
    "    if i in node2vec.keys() and j in node2vec.keys():\n",
    "        dot1 = np.dot(node2vec[i], node2vec[j])\n",
    "        random_node = random.sample(nodes, 1)[0]\n",
    "        while random_node == j or random_node not in node2vec.keys():\n",
    "            random_node = random.sample(nodes, 1)[0]\n",
    "        dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
    "        if dot1 > dot2:\n",
    "            a += 1\n",
    "        elif dot1 == dot2:\n",
    "            a += 0.5\n",
    "        b += 1\n",
    "\n",
    "print(\"Auc value:\", float(a) / b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc value: 0.5998865820232507\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "node2vec = {}\n",
    "\n",
    "f = open('Foursquare/tadw_embed.txt', 'rb')\n",
    "for i, j in enumerate(f):\n",
    "    if j.decode() != '\\n':\n",
    "        node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
    "        \n",
    "f1 = open(os.path.join('Foursquare/reorder_test.txt'), 'rb')\n",
    "edges = [list(map(int, i.strip().decode().split('\\t'))) for i in f1]\n",
    "nodes = list(set([i for j in edges for i in j]))\n",
    "a = 0\n",
    "b = 0\n",
    "for i, j in edges:\n",
    "    if i in node2vec.keys() and j in node2vec.keys():\n",
    "        dot1 = np.dot(node2vec[i], node2vec[j])\n",
    "        random_node = random.sample(nodes, 1)[0]\n",
    "        while random_node == j or random_node not in node2vec.keys():\n",
    "            random_node = random.sample(nodes, 1)[0]\n",
    "        dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
    "        if dot1 > dot2:\n",
    "            a += 1\n",
    "        elif dot1 == dot2:\n",
    "            a += 0.5\n",
    "        b += 1\n",
    "\n",
    "print(\"Auc value:\", float(a) / b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
